rm(list=ls())
rm(list=ls()) # clear all
led <- read.csv("data/led/phpSj3fWL.csv") # raw data
led <- read.csv("Data/phpSj3fWL.csv") # raw data
led <- read.csv("Data/phpSj3fWL.csv") # raw data
rm(list=ls()) # clear all
led <- read.csv("Data/phpSj3fWL.csv") # raw data
# recode the 5 features from 1/0 to Vi-1/Vi-0 (i /in {1,...,5})
for (i in 1:5) {
led[,i] <- recode(led[,i], `1` = paste0(colnames(led)[i], "-1"), `0` = paste0(colnames(led)[i], "-0"))
}
?recode
??recode
rm(list=ls()) # clear all
###################################################################################
########### Example data 2 - five explanatory variable
###################################################################################
# load package
library(tidyverse)
led <- read.csv("Data/phpSj3fWL.csv") # raw data
# recode the 5 features from 1/0 to Vi-1/Vi-0 (i /in {1,...,5})
for (i in 1:5) {
led[,i] <- recode(led[,i], `1` = paste0(colnames(led)[i], "-1"), `0` = paste0(colnames(led)[i], "-0"))
}
# split the data
set.seed(24) # seed
indice <- sample(1:nrow(led), 0.8 * nrow(led), replace = F) # indices for test data (20% obs)
training <- led[indice,] # training set
test <- led[-indice,] # test set
write_csv(training, "Data/traindata_2.csv") # training set
write_csv(test, "Data/testdata_2.csv") # test set
write_csv(led, "Data/data_2.csv") # the whole data (recoded)
rm(list=ls())
######################################################################################
############# LBA-NN Implementation on Example 1
######################################################################################
rm(list=ls())
# Load packages and functions
source("Script/4. function.R")
library(tensorflow)
library(lbann)
library(keras)
library(tidyverse)
library(caret)
library(cvms)
######################################################################################
############# linear data - one variable
######################################################################################
# load data
train <- read.csv("Data/traindata_2.csv") # training
test <- read.csv("Data/testdata_2.csv") # test
test_x <- evm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # input data matrix for the test set
test_y <- rvm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # output data matrix for the test set
#################################### Set LBA-NN
# build the model
set_random_seed(10)  # seed
lbann.model <- lbann(Class ~ V1 + V2 + V3 + V4 + V5,
data = train, num.layer = 128, epochs = 50,
val_split.ratio = 0, interaction = F, lr = 0.0001, K = 8)
#################################### Set LBA-NN
# build the model
set_random_seed(10)  # seed
lbann.model <- lbann(Class ~ V1 + V2 + V3 + V4 + V5,
data = train, num.neurons = 128, epochs = 50,
val_split.ratio = 0, interaction = F, lr = 0.0001, K = 8)
#################################### qualitative evaluation
# Importance plot
lbann.model$importance.plot
ggsave("Output/fig6_importance_plot_example_2.png")
# LBA-NN-K-means
lbann.model$biplot
ggsave("Output/7_biplot_example_2.png")
#################################### quantitative evaluation
## mean sqaure error
mse <- lbann.model$model %>% keras::evaluate(test_x, test_y) # mse = 0.07
mse
## accuracy + precision + recall + specificity + f1-score
# Confusion matrix result
y_pred.lbann <- predict(lbann.model$model, x = test_x) # predicted Y
y_pred.lbann <- apply(y_pred.lbann, 1, function(x) {colnames(lbann.model$output.matrix)[which.max(x)]}) # predicted class
## accuracy + precision + recall + specificity + f1-score
# Confusion matrix result
y_pred.lbann <- predict(lbann.model$model, x = test_x) # predicted Y
y_pred.lbann <- apply(y_pred.lbann, 1, function(x) {colnames(lbann.model$output.matrix)[which.max(x)]}) # predicted class
cm <- confusionMatrix(table(predict = y_pred.lbann, true = as.character(test$Class))) # summary: accuracy = 0.75
acc <- as.matrix(cm,what="overall")[1]
var <- c("Pos Pred Value","Recall", "Specificity", "F1") # all indicators
cm <- as.matrix(cm, what = "classes")[var,] # extract used indicators
cm.sum <- data.frame(LBANN = round(c(mse[1], acc, apply(cm, 1, mean)),2))
row.names(cm.sum) <- c("mean square error","accuracy", "precision", "recall", "specificity", "f1-score")
cm.sum
write.csv(cm.sum, "Output/tab8_1_summary_prediction_lbann.csv") # table 5 (LBA part: summary of the performance)
# draw the confusion matrix
eval <- cvms::evaluate(
data = data.frame(predict = as.character(y_pred.lbann), true = as.character(test$Class)),
target_col = "true",
prediction_cols = "predict",
type = "multinomial"
)
png("Output/fig8a_confusion_matrix_lbann.png")
plot_confusion_matrix(eval)
dev.off()
######################################################################################
############# LBA Implementation on Example 1
######################################################################################
rm(list=ls())
# Load packages and functions
source("Script/4. function.R")
library(lba)
library(tidyverse)
library(caret)
library(cvms)
######################################################################################
############# linear data - one variable
######################################################################################
# load data
train <- read.csv("Data/traindata_2.csv") # training
test <- read.csv("Data/testdata_2.csv") # test
data <- read.csv("Data/data_2.csv")
test_x <- evm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # input data matrix for the test set
test_y <- rvm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # output data matrix for the test set
### contingency table
contable <- rbind(table(V1 = data$V1, class = data$Class),
table(V2 = data$V2, class = data$Class),
table(V3 = data$V3, class = data$Class),
table(V4 = data$V4, class = data$Class),
table(V5 = data$V5, class = data$Class))
write.csv(contable, "Output/tab6_contingency_table_example2.csv") # table 3: contingency table of the data
#################################### LBA
set.seed(1)
lba.model <- lba(Class~V1 + V2 + V3 + V4 + V5, data = train, K = 8,
method = "ls", trace.lba = F) # build LBA with 3 latent budgets
#################################### qualitative evaluation
write.csv(round(lba.model$A, 2), "Output/tab7_1_mixing parameters A.csv")
write.csv(round(lba.model$B, 2), "Output/tab7_2_latent budgets B.csv")
#################################### quantitative evaluation
y_pred.lba <- test_x %*% lba.model$A %*% t(lba.model$B) # predicted Y
## mean sqaure error
mse <- mean((test_y - y_pred.lba)^2) # mse = 0.11
mse
## accuracy + precision + recall + specificity + f1-score
# Confusion matrix result
colnames(y_pred.lba) <- row.names(lba.model$B)
y_pred.lba <- as.numeric(apply(y_pred.lba, 1, function(x) {colnames(y_pred.lba)[which.max(x)]}))  # predicted class
## accuracy + precision + recall + specificity + f1-score
# Confusion matrix result
colnames(y_pred.lba) <- row.names(lba.model$B)
#################################### quantitative evaluation
y_pred.lba <- test_x %*% lba.model$A %*% t(lba.model$B) # predicted Y
## mean sqaure error
mse <- mean((test_y - y_pred.lba)^2) # mse = 0.23
mse
## accuracy + precision + recall + specificity + f1-score
# Confusion matrix result
colnames(y_pred.lba) <- row.names(lba.model$B)
y_pred.lba <- as.numeric(apply(y_pred.lba, 1, function(x) {colnames(y_pred.lba)[which.max(x)]}))  # predicted class
cm <- confusionMatrix(as.factor(y_pred.lba), reference = as.factor(test$Class)) # accuracy = 0.64
acc <- as.matrix(cm,what="overall")[1] # acc
var <- c("Pos Pred Value","Recall", "Specificity", "F1") # all indicators
cm <- as.matrix(cm, what = "classes")[var,] # extract used indicators
cm.sum <- data.frame(LBA = round(c(mse[1], acc, apply(cm, 1, mean)),2))
row.names(cm.sum) <- c("mean square error","accuracy", "precision", "recall", "specificity", "f1-score")
write.csv(cm.sum, "Output/tab8_2_summary_prediction_lba.csv") # table 5 (LBA part: summary of the performance)
# draw the confusion matrix
eval <- evaluate(
data = data.frame(predict = as.character(y_pred.lba), true = as.character(test$Class)),
target_col = "true",
prediction_cols = "predict",
type = "multinomial"
)
png("Output/fig8b_confusion_matrix_lba.png") # figure 5b
plot_confusion_matrix(eval)
dev.off()
rm(list=ls())
# Load packages and functions
library(tensorflow)
library(keras)
######################################################################################
############# LBA-NN Implementation on Example 1
######################################################################################
rm(list=ls())
# Load packages and functions
source("Script/4. function.R")
library(tensorflow)
library(lbann)
library(keras)
library(tidyverse)
library(caret)
library(cvms)
######################################################################################
############# linear data - one variable
######################################################################################
# load data
train <- read.csv("Data/traindata_2.csv") # training
test <- read.csv("Data/testdata_2.csv") # test
test_x <- evm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # input data matrix for the test set
test_y <- rvm.test(Class ~ V1 + V2 + V3 + V4 + V5,
newdata = test, trainingset = train) # output data matrix for the test set
#################################### Set LBA-NN
# build the model
set_random_seed(10)  # seed
lbann.model <- lbann(Class ~ V1 + V2 + V3 + V4 + V5,
data = train, num.neurons = 128, epochs = 50,
val_split.ratio = 0, interaction = F, lr = 0.0001, K = 8)
#################################### qualitative evaluation
# Importance plot
lbann.model$importance.plot
ggsave("Output/fig6_importance_plot_example_2.png")
# LBA-NN-K-means
lbann.model$biplot
ggsave("Output/fig7_biplot_example_2.png")
